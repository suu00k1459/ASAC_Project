{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v5KUORAyQ4rh","executionInfo":{"status":"ok","timestamp":1719195862692,"user_tz":-540,"elapsed":20202,"user":{"displayName":"suyoung kim","userId":"00578649688239691521"}},"outputId":"591e8c32-0048-42b7-e1fe-b1f132ad86cc"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import torch\n","import torchvision\n","from torchvision.models.detection import maskrcnn_resnet50_fpn\n","import cv2\n","import numpy as np\n","import os\n","\n","# 사전 학습된 Mask R-CNN 모델 로드\n","model = maskrcnn_resnet50_fpn(pretrained=True)\n","model.eval()  # 모델을 평가 모드로 설정"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vp_U3EL-RCpT","executionInfo":{"status":"ok","timestamp":1719195897465,"user_tz":-540,"elapsed":1348,"user":{"displayName":"suyoung kim","userId":"00578649688239691521"}},"outputId":"94286860-8e92-4b99-9279-81132d74b41b","collapsed":true},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MaskRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): FrozenBatchNorm2d(256, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(512, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(1024, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(2048, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (layer_blocks): ModuleList(\n","        (0-3): 4 x Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","      )\n","      (extra_blocks): LastLevelMaxPool()\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","      )\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n","    )\n","    (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n","    (mask_head): MaskRCNNHeads(\n","      (0): Conv2dNormActivation(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","      )\n","      (1): Conv2dNormActivation(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","      )\n","      (2): Conv2dNormActivation(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","      )\n","      (3): Conv2dNormActivation(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","      )\n","    )\n","    (mask_predictor): MaskRCNNPredictor(\n","      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n","      (relu): ReLU(inplace=True)\n","      (mask_fcn_logits): Conv2d(256, 91, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# Haar Cascade 분류기 파일 경로\n","face_cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n","\n","# Haar Cascade 분류기 로드\n","face_cascade = cv2.CascadeClassifier(face_cascade_path)\n","\n","for i in range(0,12):\n","    # 기존 이미지 로드\n","    original_image_path = f'/content/drive/MyDrive/3조/weddingdataset/wedding_test/image/{60000 + i}_00.jpg'\n","\n","    # 이미지 파일이 존재하는지 확인\n","    if not os.path.exists(original_image_path):\n","        print(f\"File {original_image_path} does not exist.\")\n","        continue\n","\n","    original_image = cv2.imread(original_image_path)\n","    original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n","\n","    # 회색 이미지 추출\n","    image_tensor = torchvision.transforms.functional.to_tensor(original_image_rgb).unsqueeze(0)\n","\n","    # 모델 예측 수행\n","    with torch.no_grad():\n","        prediction = model(image_tensor)[0]\n","\n","    # 결과 확인 (마스크와 클래스 ID 추출)\n","    masks = prediction['masks'].cpu().numpy()\n","    labels = prediction['labels'].cpu().numpy()\n","    scores = prediction['scores'].cpu().numpy()\n","\n","    # 클래스 ID가 1인 마스크 필터링 (예: 사람)\n","    person_mask = np.zeros((original_image_rgb.shape[0], original_image_rgb.shape[1]), dtype=np.uint8)\n","    for j in range(masks.shape[0]):\n","        if labels[j] == 1 and scores[j] > 0.5:  # 신뢰도가 0.5 이상인 경우만 사용\n","            person_mask = np.maximum(person_mask, masks[j, 0])\n","\n","    # 마스크 확장\n","    kernel = np.ones((30, 30), np.uint8)  # 커널 크기를 조정하여 확장 범위 조절\n","    dilated_mask = cv2.dilate(person_mask, kernel, iterations=1)\n","\n","    # 하얀색과 검정색 정의\n","    white_color = (255, 255, 255)  # 하얀색\n","    black_color = (0, 0, 0)  # 검정색\n","\n","    # 모든 영역을 검정색으로 채우기\n","    processed_image = np.full_like(original_image_rgb, black_color, dtype=np.uint8)\n","\n","    # 확장된 영역을 하얀색으로 채우기\n","    processed_image[dilated_mask > 0.5] = white_color\n","\n","    # 얼굴 감지\n","    gray = cv2.cvtColor(original_image_rgb, cv2.COLOR_RGB2GRAY)\n","    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n","\n","    if len(faces) > 0:\n","        # 얼굴 위치가 가장 높은 것을 찾기\n","        highest_face = min(faces, key=lambda rect: rect[1])\n","        x, y, w, h = highest_face\n","\n","        # 얼굴 중심과 반경 설정\n","        center_x, center_y = x + w // 2, y + h // 2\n","\n","        # 너비와 높이에 각각 다른 배율 적용\n","        adjusted_w = int(1.8 * w)\n","        adjusted_h = int(1.2 * h)\n","\n","        # 타원형 마스크 생성\n","        mask = np.zeros_like(original_image_rgb)\n","        mask = cv2.ellipse(mask, (center_x, center_y), (adjusted_w // 2, adjusted_h // 2), 0, 0, 360, (255, 255, 255), -1)\n","\n","        # 얼굴 마스크 영역을 검정으로 채우기\n","        processed_image[mask[:, :, 0] == 255] = black_color\n","\n","        # 중앙을 기준으로 윗부분을 검정으로 채우기\n","        processed_image[:center_y, :] = black_color\n","\n","    # 결과 이미지 저장 (RGB 색상 공간으로 변환하여 저장)\n","    output_path = f'/content/drive/MyDrive/3조/weddingdataset/agnostic-mask/{60000 + i}_00_mask.png'\n","    cv2.imwrite(output_path, cv2.cvtColor(processed_image, cv2.COLOR_RGB2BGR))"],"metadata":{"id":"SCRteUauQ5xE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Haar Cascade 분류기 파일 경로\n","face_cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n","\n","# Haar Cascade 분류기 로드\n","face_cascade = cv2.CascadeClassifier(face_cascade_path)\n","\n","# 기존 이미지 폴더 경로\n","input_folder = '/content/drive/MyDrive/3조/weddingdataset/viton-hd-idm/train/image'\n","output_folder = '/content/drive/MyDrive/3조/weddingdataset/viton-hd-idm/train/agnostic-mask'\n","\n","# 디렉토리 내 모든 파일 이름을 가져오기\n","file_names = os.listdir(input_folder)\n","\n","for file_name in file_names:\n","    # 기존 이미지 경로\n","    original_image_path = os.path.join(input_folder, file_name)\n","\n","    # 이미지 파일이 존재하는지 확인\n","    if not os.path.exists(original_image_path):\n","        print(f\"File {original_image_path} does not exist.\")\n","        continue\n","\n","    # 이미지 로드\n","    original_image = cv2.imread(original_image_path)\n","    original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n","\n","    # 회색 이미지 추출\n","    image_tensor = torchvision.transforms.functional.to_tensor(original_image_rgb).unsqueeze(0)\n","\n","    # 모델 예측 수행\n","    with torch.no_grad():\n","        prediction = model(image_tensor)[0]\n","\n","    # 결과 확인 (마스크와 클래스 ID 추출)\n","    masks = prediction['masks'].cpu().numpy()\n","    labels = prediction['labels'].cpu().numpy()\n","    scores = prediction['scores'].cpu().numpy()\n","\n","    # 클래스 ID가 1인 마스크 필터링 (예: 사람)\n","    person_mask = np.zeros((original_image_rgb.shape[0], original_image_rgb.shape[1]), dtype=np.uint8)\n","    for j in range(masks.shape[0]):\n","        if labels[j] == 1 and scores[j] > 0.5:  # 신뢰도가 0.5 이상인 경우만 사용\n","            person_mask = np.maximum(person_mask, masks[j, 0])\n","\n","    # 마스크 확장\n","    kernel = np.ones((30, 30), np.uint8)  # 커널 크기를 조정하여 확장 범위 조절\n","    dilated_mask = cv2.dilate(person_mask, kernel, iterations=1)\n","\n","    # 하얀색과 검정색 정의\n","    white_color = (255, 255, 255)  # 하얀색\n","    black_color = (0, 0, 0)  # 검정색\n","\n","    # 모든 영역을 검정색으로 채우기\n","    processed_image = np.full_like(original_image_rgb, black_color, dtype=np.uint8)\n","\n","    # 확장된 영역을 하얀색으로 채우기\n","    processed_image[dilated_mask > 0.5] = white_color\n","\n","    # 얼굴 감지\n","    gray = cv2.cvtColor(original_image_rgb, cv2.COLOR_RGB2GRAY)\n","    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n","\n","    if len(faces) > 0:\n","        # 얼굴 위치가 가장 높은 것을 찾기\n","        highest_face = min(faces, key=lambda rect: rect[1])\n","        x, y, w, h = highest_face\n","\n","        # 얼굴 중심과 반경 설정\n","        center_x, center_y = x + w // 2, y + h // 2\n","\n","        # 너비와 높이에 각각 다른 배율 적용\n","        adjusted_w = int(1.8 * w)\n","        adjusted_h = int(1.2 * h)\n","\n","        # 타원형 마스크 생성\n","        mask = np.zeros_like(original_image_rgb)\n","        mask = cv2.ellipse(mask, (center_x, center_y), (adjusted_w // 2, adjusted_h // 2), 0, 0, 360, (255, 255, 255), -1)\n","\n","        # 얼굴 마스크 영역을 검정으로 채우기\n","        processed_image[mask[:, :, 0] == 255] = black_color\n","\n","        # 중앙을 기준으로 윗부분을 검정으로 채우기\n","        processed_image[:center_y, :] = black_color\n","\n","    # 파일 이름 변경 (뒤에 _mask 추가하고 확장자 png로 변경)\n","    base_name, _ = os.path.splitext(file_name)\n","    output_file_name = f\"{base_name}_mask.png\"\n","    output_path = os.path.join(output_folder, output_file_name)\n","\n","    # 결과 이미지 저장 (RGB 색상 공간으로 변환하여 저장)\n","    cv2.imwrite(output_path, cv2.cvtColor(processed_image, cv2.COLOR_RGB2BGR))"],"metadata":{"id":"ZoPclkVcQ8Ft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nEjpKE8nRtFX"},"execution_count":null,"outputs":[]}]}